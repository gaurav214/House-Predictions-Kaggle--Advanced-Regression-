# Introduction
Kaggle describes this competition as follows:

Ask a home buyer to describe their dream house, and they probably won’t begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition’s dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.

With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.

# Practice Skills
 1. Creative feature engineering 
 2. Advanced regression techniques like random forest and gradient boosting

# Flow of Approach
 1. Importing Dependencies
 2. Loading and Inspecting Dataset
 3. Data Cleaning
 4. Preparing the Data
 5. Feature Creation
 6. Feature Scaling
 7. Encoding Categorical variables
 8. Regression Model Creation
 9. Model optimization
 10. Repeating 8 and 9 for different models
 
 # Different Models Used:
  1. RandomForestRegressor
  2. RandomForestRegressor(Grid Search)
  3. SupportVectorRegressor
  
  ***Currently Adding More models to achive highest accuracy***

### References
1. https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data
2. https://www.kaggle.com/erikbruin/house-prices-lasso-xgboost-and-a-detailed-eda
3. https://brownmath.com/stat/shape.htm
6. https://www.youtube.com/watch?v=XdM6ER7zTLk
